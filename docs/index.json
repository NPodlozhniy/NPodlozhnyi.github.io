[{"content":"\rBackground Every company has always taken pride in providing excellent customer service, therefore it\u0026rsquo;s crucial as part of ongoing improvements for product to collect and analyse feedback after each customer interaction on support channels\nA top-notch analyst in Hi-Tech industry should be in capable of handle at least base feedback analysis quickly and efficiently and going through this article you will be introduced with the necessary concepts and eventually given the base guide on how to unlock the true power of text data\nWell, no more words, let\u0026rsquo;s have a look on how\nDisclaimer: I\u0026rsquo;m not pretending on having this article as an exhaustive everything you need notebook for analyzing customer feedback, but it\u0026rsquo;s the steps I usually follow while working on customers text data and on the basis of my previous experience - 80% of your needs is covered here\nPrerequisites It\u0026rsquo;s expected that the reader has an experience with Python and it\u0026rsquo;s main Data Analysis libraries\nThe actual notebook was written on Python 3.7.9 and to keep the results reproducible here is the list of particular packages that are being used in the article specified with their versions\nCode\rnumpy==1.21.6 pandas==1.3.5 pandas-profiling==3.1.0 matplotlib==3.4.2 unidecode==1.3.7 nltk==3.6.2 sklearn==0.24.2 wordcloud==1.9.2 shap==0.42.1 transformers==4.30.2 gensim==4.0.1 Methodology During this notebook the feedback which comes in the form of ratings (from 1-5) and textual comments is considered. The general purpose is to dive deeper into this feedback, identify common themes, if certain issues lead to more negative feedback than others and understand areas of improvement\nCode\rimport numpy as np import pandas as pd import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) from matplotlib import pyplot as plt %matplotlib inline As a general rule the analysis should be organized in a top-down manner, simple exploration and low-hanging fruits first and sophisticated approach after only if you really need more in-depth expertise. According to the above the analysis will be organized in the three steps:\nData Mining - simple unsupervised data exploration to have a general idea of the data nature Sentiment Analysis - search on what matters the most which reasons drive review to be positive or negative Topic Modelling - main feedback themes extraction, improvement focuses identification Analysis Process Data Mining Basic EDA First of all - let\u0026rsquo;s have a fluent look at the suggested data, pandas-profiling is a very useful tool to perform basic and boring Exploratory Data Analysis in a minute. View Data Profile by clicking here\nCode\rfrom pandas_profiling import ProfileReport df = pd.read_csv(\u0026#39;feedback-data-sample.csv\u0026#39;, index_col=0) profile = ProfileReport( df, minimal=True, dark_mode=True, title=\u0026#34;Feedback Data Report\u0026#34;, ) profile.to_file(\u0026#34;data-profile.html\u0026#34;) Well, what are the main data patterns\nthe dataset constitutes 1276 tickets with customer feedback consisted of csat_score and comment ticket_id - is a primary key, given that we don\u0026rsquo;t have timestamp column and it\u0026rsquo;s looks like bigint not a random hash, it\u0026rsquo;s better to sort by it beforehand, to be sure we are not predicting the past on the future features all the reviews are in English language, good for us, we can forget about additional translators and this column in general csat_score has only two different values 0 and 4 stars, and the majority of reviews are positive, well not much, but it\u0026rsquo;s even easier to transform it into a bool target and work with it further comment has a few NULL values, let\u0026rsquo;s keep it in mind from the simple frequentist analysis it\u0026rsquo;s clear that tokens not and very might be informative, so don\u0026rsquo;t forget to exclude them from stop words list Code\rdf = df.sort_index().drop(columns={\u0026#39;language\u0026#39;}) def define_sentiment(rating: float) -\u0026gt; int: if rating \u0026lt; 3: return -1 # negative sentiment elif rating \u0026gt; 3: return 1 # positive sentiment else: return 0 # neutral sentiment df[\u0026#39;sentiment\u0026#39;] = df[\u0026#39;csat_score\u0026#39;].apply(define_sentiment) Text cleaning Barely the role of data cleaning might be underestimated, it\u0026rsquo;s an incredibly important step, if this is skipped the rest of analysis doesn\u0026rsquo;t make any sense then.\nMain cleaning that should be applied:\nremove all the symbols and keep only words remove redundant short words remove general language words transliterate unicode symbols to ascii lowercase The next step is tokenization: the are two main approaches here:\nstemming - fast process of removing prefixes and suffixes to give a word a short form, that might not be a dictionary word though lemmatization - finds meaningful word representation from dictionary and depends on the part-of-speech To summarize, the stemming is just searching for a common ground between words and cutting ends then and therefore it takes less time whereas lemmatization provides better results by performing a specific morphological analysis and produces a real word which is extremely important for some human-interactive applications\nSounds like if the resources are not a problem it\u0026rsquo;s better to use lemmatization by default, but there is an opinion that Stemming works efficiently for some specific tasks like: spam classification and feedback sentiment classification, given that it\u0026rsquo;s the case, let\u0026rsquo;s apply both and take a choice in the end\nCode\rimport re import nltk # If the code below doesn\u0026#39;t work - download add-ons first # nltk.download([\u0026#39;stopwords\u0026#39;, \u0026#39;wordnet\u0026#39;]) from nltk.corpus import stopwords stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) # this words might be useful, better to retain for now for word in [\u0026#39;very\u0026#39;, \u0026#39;not\u0026#39;]: stop_words.remove(word) from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() # better version of the Porter Stemmer from nltk.stem.snowball import SnowballStemmer stemmer = SnowballStemmer(language=\u0026#34;english\u0026#34;) for example in [\u0026#39;programming\u0026#39;, \u0026#39;quickly\u0026#39;, \u0026#39;very\u0026#39;]: print(f\u0026#34;Example: {example}\u0026#34;) print(f\u0026#34; - Lemma: {lemmatizer.lemmatize(example, pos=\u0026#39;v\u0026#39;)}\u0026#34;) print(f\u0026#34; - Stem: {stemmer.stem(example)}\u0026#34;) Example: programming\r- Lemma: program\r- Stem: program\rExample: quickly\r- Lemma: quickly\r- Stem: quick\rExample: very\r- Lemma: very\r- Stem: veri\rUnicode transliteration is needed, given that the data contains non-ascii symbols\nCode\rfrom unidecode import unidecode df[df.comments.notnull()][ df[df.comments.notnull()].comments != df[df.comments.notnull()].comments.apply(unidecode) ].comments.sample(5) ticket_id\r43532202076873 Just asking me to show what I‚Äôm doing to give ...\r43532202076331 I haven‚Äôt had a reply\r43532202117219 All good üëç thanks\r43532202073174 Chat went silent. After talking to someone the...\r43532202202546 Hi Val√©ria,\\nIt still does not work. There mus...\rName: comments, dtype: object\rPutting it all together\nCode\rdef clean_data(x, tokenizer, black_list=stop_words): \u0026#34;\u0026#34;\u0026#34; The method removes from a sentence `x` - punctuation \u0026amp; digits - too short words (less than 3 letters) - unicode symbols (translate to ascii) - words from `black_list` Return lowercased and tokenized text \u0026#34;\u0026#34;\u0026#34; words = re.findall(\u0026#39;\\w{3,}\u0026#39;, re.sub(\u0026#39;[^a-z√Ä-√ø ]\u0026#39;, \u0026#39; \u0026#39;, str(x).lower() if x is not np.NaN else \u0026#39;\u0026#39;)) tokens = [tokenize(unidecode(word), tokenizer) for word in words] return \u0026#39; \u0026#39;.join([word for word in tokens if word not in black_list]) def tokenize(x: str, tokenizer) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Applies either stemming or lemmatization to a token `x` \u0026#34;\u0026#34;\u0026#34; if hasattr(tokenizer, \u0026#39;lemmatize\u0026#39;): return tokenizer.lemmatize(x, pos=\u0026#39;v\u0026#39;) elif hasattr(tokenizer, \u0026#39;stem\u0026#39;): return tokenizer.stem(x) else: raise ValueError(\u0026#34;tokenizer should be either Lemmatizer or Stemmer\u0026#34;) df[\u0026#34;lemma_text\u0026#34;] = df.comments.apply(clean_data, args=(lemmatizer,)) df[\u0026#34;stem_text\u0026#34;] = df.comments.apply(clean_data, args=(stemmer,)) Words Frequency There are many different ways how to tackle the visual text analysis, the popular and convenient way is Word Clouds where the size of the word reflects its frequency within the given text\nP.S. In any data mining initiative, it is a good idea to retain some portion of the data to validate your final findings, so let\u0026rsquo;s create a holdout piece of data to adhere true-to-life approach\nCode\rfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( df[[\u0026#34;lemma_text\u0026#34;, \u0026#34;stem_text\u0026#34;]], df[\u0026#39;sentiment\u0026#39;], train_size=1000, shuffle=False ) from collections import Counter def create_ngrams(tokens: list, n: int): ngrams = zip(*[tokens[idx:] for idx in range(n)]) return [\u0026#34; \u0026#34;.join(sorted(ngram)) for ngram in ngrams] def frequent_ngrams(documents: list, n: int = 1): \u0026#34;\u0026#34;\u0026#34; Use .most_common(top_n) to get top n ngrams \u0026#34;\u0026#34;\u0026#34; ngrams = [] if n == 1: ngrams = \u0026#34; \u0026#34;.join(list(documents)).split() elif n \u0026gt;= 2: for tokens in documents: ngrams.extend(create_ngrams(tokens.split(), n)) else: raise ValueError(\u0026#34;n for n-grams should be a positive number\u0026#34;) return Counter(ngrams) import wordcloud def make_word_cloud(text, stop_words=None): plt.figure(figsize=(12, 9)) kwargs = { \u0026#39;width\u0026#39;: 1600, \u0026#39;height\u0026#39;: 900, \u0026#39;min_font_size\u0026#39;: 10 } if isinstance(text, str): word_cloud = wordcloud.WordCloud(stopwords=stop_words, **kwargs).generate_from_text(text) elif isinstance(text, list) or isinstance(text, np.ndarray): word_cloud = wordcloud.WordCloud(stopwords=stop_words, **kwargs).generate(\u0026#34; \u0026#34;.join(text)) else: if stop_words: text = {word: value for word, value in text.items() if word not in stop_words} word_cloud = wordcloud.WordCloud(**kwargs).generate_from_frequencies(text) plt.imshow(word_cloud) plt.axis(\u0026#34;off\u0026#34;) plt.show() First of all, as promised different tokenizers should be compared\nCode\rmake_word_cloud(frequent_ngrams(X_train[\u0026#34;stem_text\u0026#34;], 1)) Code\rmake_word_cloud(frequent_ngrams(X_train[\u0026#34;lemma_text\u0026#34;], 1)) Well, at first glance it looks like both tokenizers work very similarly, one noticeable difference is that stemmer treats word pairs like help and helpful or quick and quickly as one token and actually it might be wrong, imagine if we encounter helpful more in positive sentence and help in negative, then they shouldn\u0026rsquo;t be united\nCode\rfor word in [\u0026#34;help\u0026#34;, \u0026#34;helpful\u0026#34;]: score = df.loc[ df[\u0026#34;comments\u0026#34;].apply( lambda x: f\u0026#34; {word} \u0026#34; in x if x is not np.nan else False ), \u0026#34;sentiment\u0026#34; ].mean() print(f\u0026#34;for `{word}` average sentiment score = {score:.2f}\u0026#34;) for `help` average sentiment score = 0.31\rfor `helpful` average sentiment score = 0.95\rIndeed, that is the case, so let\u0026rsquo;s end up with traditional lemmatizer and take a look at word clouds separately for different sentiments\nCode\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026lt; 0, \u0026#34;lemma_text\u0026#34;], 1)) Code\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026gt; 0, \u0026#34;lemma_text\u0026#34;], 1)) Good news, they are very different in essence, in positive reviews customers use gratitude words more like thank and helpful on the other hand in negative sentences customer highlight that the issue still not resolved. In addition it might be useful to take a look at popular collocations\nCode\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026gt; 0, \u0026#34;lemma_text\u0026#34;], 2)) Code\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026lt; 0, \u0026#34;lemma_text\u0026#34;], 3)) N-grams appear to be very informative:\nin positive sentences, based on bigrams, customers say that the response was quick, problem was solved and the support was very helpful in negative sentences, on the basis of trigrams, customers claim that the issue/problem not resolved sometimes the add yet or still to fully express their dissatisfaction To summarize, the basic approach has already given some meaningful insights and the hope that the reviews might be classified automatically quite well and themes can be modelled then\nUnsupervised TF-IDF It\u0026rsquo;s critical to reduce feature number otherwise X-matrix will be too sparse, and the clusterization will fail to give a substantial result, actually only quite often words should be taken into account\nCode\rfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score from sklearn.cluster import KMeans min_word_counts = range(1, 100, 2) n_features, scores = [], [] for min_word_count in min_word_counts: tfidf = TfidfVectorizer(min_df=min_word_count) X = tfidf.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) n_features.append(X.shape[1]) km = KMeans( n_clusters=2, init=\u0026#39;k-means++\u0026#39;, max_iter=300, random_state=20231020 ) km.fit(X) score = accuracy_score(2 * km.predict(X) - 1, y_train) scores.append( max(score, 1 - score) ) plt.style.use(\u0026#39;ggplot\u0026#39;) fig, ax1 = plt.subplots(figsize=(12, 6)) ax2 = ax1.twinx() ax2.bar(min_word_counts, n_features, color=\u0026#34;b\u0026#34;, width=1.5, alpha=0.33) ax1.plot(min_word_counts, scores, \u0026#34;g--\u0026#34;, linewidth=2) ax1.set_ylabel(\u0026#39;Accuracy, %\u0026#39;, color=\u0026#34;g\u0026#34;) ax1.set_xlabel(\u0026#39;Minimal Word Frequency, #\u0026#39;) ax2.set_ylabel(\u0026#39;N Features, #\u0026#39;, color=\u0026#34;b\u0026#34;) plt.title(\u0026#39;K-Means Clusterization\u0026#39;) plt.show() From the chart is clear that the larger number of features doesn\u0026rsquo;t lead to the Accuracy increase, from the principal of maximum Accuracy the optimal number of features might be 69 for example, let\u0026rsquo;s fix it and track which features the model decided to consider\nCode\rtfidf = TfidfVectorizer(min_df=69) X = tfidf.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) words = np.array(tfidf.get_feature_names()) print(f\u0026#34;Number of features: {X.shape[1]}, namely: \u0026#34;) print(*words) Number of features: 14, namely: answer get help helpful issue not problem quick resolve response solve still thank very\rThe clusterization which is based just on 14 words! gives an accuracy higher than 75%, but it\u0026rsquo;s the result only valid for the train sample, which was used to identify min_df hyperparameter, so to have an unbiased estimation test sample should be considered here\nCode\rkm = KMeans( n_clusters=2, init=\u0026#39;k-means++\u0026#39;, max_iter=300, random_state=20231020 ) km.fit(X) centroids_important_indexes = km.cluster_centers_.argsort()[:,::-1] for idx in range(km.get_params()[\u0026#39;n_clusters\u0026#39;]): print(\u0026#34;Cluster No.\u0026#34;, idx, *words[centroids_important_indexes[idx, :7]]) Cluster No. 0 very issue thank helpful quick resolve response\rCluster No. 1 not issue resolve still solve problem get\rWell, looks like Cluster-0 caught positive feedback and Cluster-1 negative, then given that target is the value from [-1, 1] set, to define an accuracy some transformation must be put in place first\nCode\rpredictions_train = 2 * -km.predict(X) + 1 print(f\u0026#34;Accuracy of K-Means train sample = {accuracy_score(predictions_train, y_train):.1%}\u0026#34;) Accuracy of K-Means train sample = 76.4%\rCode\rpredictions_test = 2 * -km.predict( tfidf.transform( X_test[\u0026#34;lemma_text\u0026#34;] ) ) + 1 print(f\u0026#34;Accuracy of K-Means test sample = {accuracy_score(predictions_test, y_test):.1%}\u0026#34;) Accuracy of K-Means test sample = 74.6%\rSplendid, this toy example gives a clue that it\u0026rsquo;s pretty good approach when you need to classify your customer\u0026rsquo;s feedback while you don\u0026rsquo;t have any ratings (only text comments). Unsupervised approach based on TF-IDF Vectorizer and K-Means gives a nice baseline, but fortunately, it\u0026rsquo;s not our case let\u0026rsquo;s go ahead and use rating set by a customer in addition to texts to reach the text data potential\nSentiment Analysis The goal of this part of the article is to enhance the unsupervised model and build a powerful classifier to eventually understand key drivers for review to be positive or negative from features extraction\nThere are 2 main ways of doing Semantic Analysis:\ntrain your own model using the available data use pre-trained deep learning models and fine-tune them if needed for this particular text specific Both approaches is considered below\nCustom Regression Model Training of the custom model will be held in 3 steps (again?)\nModel architecture selection Selected model training and cross-validation Feature analysis and general evaluation Well, by the model term stands combination of Vectorizer, which transform text data into a vector representation and Classifier that is training on these vectors to predict sentiment\nOut of vectorizers we are going to try both most popular options: Classic Counter and TF-iDF, for classificators let\u0026rsquo;s search among classic linear, tree-based and in addition naive bias method, which might be extremely useful for text classification\nIn addition as it was shown during unsupervised analysis, barely all the words should be taken into account to build a substantial model and it alleviates the learning process also, therefore SVD application for feature space reduction will be considered\nCode\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.svm import LinearSVC from sklearn.decomposition import TruncatedSVD from sklearn.model_selection import cross_val_score from sklearn.pipeline import Pipeline from typing import List, Union, Optional def model(vectorizer, classifier, transformer=None): if transformer: return Pipeline([ (\u0026#34;vectorizer\u0026#34;, vectorizer), (\u0026#34;transformer\u0026#34;, transformer), (\u0026#34;classifier\u0026#34;, classifier) ]) else: return Pipeline([ (\u0026#34;vectorizer\u0026#34;, vectorizer), (\u0026#34;classifier\u0026#34;, classifier) ]) def get_entity_name(entity: Union[List[object], Optional[object]]) -\u0026gt; str: if not isinstance(entity, List): entity = [entity] return [re.sub(\u0026#34;\u0026gt;|\u0026#39;\u0026#34;, \u0026#39;\u0026#39;, str(e)).split(\u0026#34;.\u0026#34;)[-1] for e in entity if e] Using vanilla classifiers let\u0026rsquo;s define which one suits the data better from scratch and reveal hyperparameters on the validation basis after for this candidate\nCode\rnp.random.seed(20231024) for vectorizer in [CountVectorizer, TfidfVectorizer]: for classifier in [LogisticRegression, SGDClassifier, RandomForestClassifier, LinearSVC, MultinomialNB]: transformers = [None] if vectorizer == CountVectorizer: transformers.append(TfidfTransformer()) if classifier != MultinomialNB: transformers.extend([ TruncatedSVD(n_components=100), TruncatedSVD(n_components=10), ]) for transformer in transformers: print(get_entity_name([vectorizer, classifier, transformer]), end=\u0026#34;: \u0026#34;) score = cross_val_score( model(vectorizer(), classifier(), transformer), X_train[\u0026#34;lemma_text\u0026#34;], y_train, cv=5, scoring=\u0026#39;f1\u0026#39; ).mean() print(f\u0026#34;f1-score: {score:.1%}\u0026#34;) ['CountVectorizer', 'LogisticRegression']: f1-score: 90.2%\r['CountVectorizer', 'LogisticRegression', 'TfidfTransformer()']: f1-score: 90.5%\r['CountVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=100)']: f1-score: 88.7%\r['CountVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=10)']: f1-score: 87.5%\r['CountVectorizer', 'SGDClassifier']: f1-score: 88.0%\r['CountVectorizer', 'SGDClassifier', 'TfidfTransformer()']: f1-score: 89.5%\r['CountVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.0%\r['CountVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 84.7%\r['CountVectorizer', 'RandomForestClassifier']: f1-score: 90.2%\r['CountVectorizer', 'RandomForestClassifier', 'TfidfTransformer()']: f1-score: 90.1%\r['CountVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 89.1%\r['CountVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 88.3%\r['CountVectorizer', 'LinearSVC']: f1-score: 89.3%\r['CountVectorizer', 'LinearSVC', 'TfidfTransformer()']: f1-score: 90.3%\r['CountVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=100)']: f1-score: 89.2%\r['CountVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=10)']: f1-score: 87.3%\r['CountVectorizer', 'MultinomialNB']: f1-score: 90.6%\r['CountVectorizer', 'MultinomialNB', 'TfidfTransformer()']: f1-score: 90.2%\r['TfidfVectorizer', 'LogisticRegression']: f1-score: 90.5%\r['TfidfVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=100)']: f1-score: 89.7%\r['TfidfVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=10)']: f1-score: 86.7%\r['TfidfVectorizer', 'SGDClassifier']: f1-score: 89.4%\r['TfidfVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.4%\r['TfidfVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 83.5%\r['TfidfVectorizer', 'RandomForestClassifier']: f1-score: 89.9%\r['TfidfVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.9%\r['TfidfVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 88.0%\r['TfidfVectorizer', 'LinearSVC']: f1-score: 90.3%\r['TfidfVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=100)']: f1-score: 90.7%\r['TfidfVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=10)']: f1-score: 87.4%\r['TfidfVectorizer', 'MultinomialNB']: f1-score: 90.2%\rWinners:\nTfidfVectorizer \u0026amp; LinearSVC \u0026amp; TruncatedSVD CountVectorizer \u0026amp; MultinomialNB TfidfVectorizer \u0026amp; LogisticRegression On the basis of above analysis, the better approach will be to go with the first option because:\nthere is a clear rationale to apply tf-idf over usual counter for the majority of text analysis task regression is more flexible model than Naive Bayes and highly likely that after cross-validation it can accomplish even higher accuracy reducing the feature space makes sense as it was for unsupervised learning Code\rfrom sklearn.model_selection import GridSearchCV clf = model( vectorizer=TfidfVectorizer(), classifier=LinearSVC(random_state=20231020), transformer=TruncatedSVD() ) param_grid = { \u0026#34;vectorizer__max_df\u0026#34;: [1.0, 0.15, 0.10], \u0026#34;vectorizer__min_df\u0026#34;: [1, 2, 3], \u0026#34;vectorizer__ngram_range\u0026#34;: [(1, 1), (1, 2), (1, 3)], \u0026#34;classifier__C\u0026#34;: [0.75, 1, 1.25], \u0026#34;transformer__n_components\u0026#34;: [100, 500, 1000] } search = GridSearchCV(clf, param_grid, cv=3) search.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(\u0026#34;Best parameter (CV score = %0.3f):\u0026#34; % search.best_score_) for key, value in search.best_params_.items(): print(f\u0026#34;{key}: {value}\u0026#34;) Best parameter (CV score = 0.897):\rclassifier__C: 1\rclassifier__loss: hinge\rtransformer__n_components: 1000\rvectorizer__max_df: 1.0\rvectorizer__min_df: 1\rvectorizer__ngram_range: (1, 2)\rCode\r# If you want to see all the results of the scoring use the following code # for param, score in zip( # search.cv_results_[\u0026#39;params\u0026#39;], # search.cv_results_[\u0026#39;mean_test_score\u0026#39;] # ): # print(param, score) The majority of parameters remains as by default, although some of them changed, it\u0026rsquo;s interesting that only bigrams are useful, trigrams on contrary to what we saw before from dummy analysis don\u0026rsquo;t give any additional info to regression from grid-search point of view, from given experience it\u0026rsquo;s better to retain them\nWell, given the parameters are all defined, let\u0026rsquo;s set them up and take closer look at the trained model\nCode\rfrom sklearn.metrics import classification_report clf = model( vectorizer=TfidfVectorizer(ngram_range=(1, 3)), classifier=LinearSVC(random_state=20231020), transformer=TruncatedSVD(n_components=1000), ) clf.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(classification_report(y_test, clf.predict(X_test[\u0026#34;lemma_text\u0026#34;]))) precision recall f1-score support\r-1 0.90 0.86 0.88 111\r1 0.91 0.94 0.92 165\raccuracy 0.91 276\rmacro avg 0.91 0.90 0.90 276\rweighted avg 0.91 0.91 0.91 276\rIf we just out of curiosity take a look at another candidate - Naive Bayes, then the results are\nCode\rclf = model( vectorizer=CountVectorizer(ngram_range=(1, 3)), classifier=MultinomialNB(), ) clf.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(classification_report(y_test, clf.predict(X_test[\u0026#34;lemma_text\u0026#34;]))) precision recall f1-score support\r-1 0.90 0.82 0.86 111\r1 0.89 0.94 0.91 165\raccuracy 0.89 276\rmacro avg 0.89 0.88 0.89 276\rweighted avg 0.89 0.89 0.89 276\rWell, regression works a bit better and it\u0026rsquo;s a pretty powerful classificator, but what is really needed is feature exploration, which words have been determined by the model sentiment and what do customers really appreciate or complain about. In order to evaluate features easily SVD step is skipped here, it doesn\u0026rsquo;t inflict tangible damage on model quality but simplifies analysis a lot\nCode\rimport shap vectorizer=TfidfVectorizer(ngram_range=(1, 3)) classifier=LinearSVC(random_state=20231020) X_train_vec = vectorizer.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) classifier.fit(X_train_vec, y_train) explainer = shap.Explainer( classifier, X_train_vec, feature_names=vectorizer.get_feature_names() ) shap_values = explainer(X_train_vec) shap.plots.beeswarm(shap_values, max_display=30, plot_size=(12, 8)) Well, the results almost don\u0026rsquo;t reveal any new pattens:\nclients like quick and clear answered questions, they are thankful for fast and friendly support and of course solved problem is everything clients dislike: if have no response (reply) and if the problem still not resolved (yet), in general they don\u0026rsquo;t like to wait for getting a solution However, there are some new words here - account, pleo and sonja, and there is an easy way to check how the model works for a particular review\nP.S. if visualization doesn\u0026rsquo;t work run shap.initjs() first\nCode\rfor word in[\u0026#39;account\u0026#39;, \u0026#39;pleo\u0026#39;, \u0026#39;sonja\u0026#39;]: print(word, *X_train[\u0026#34;lemma_text\u0026#34;].apply(lambda x: word in x).values.argsort()[-3:]) account 154 375 104\rpleo 386 532 242\rsonja 826 943 190\rCode\ridx = 154 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Negative\rText: my email account is still not connected and it does not work. tried a couple of times to reconnect\rCode\ridx = 242 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Positive\rText: Fast answer and a perfect answer because the function I was looking for was in Pleo :D\rCode\ridx = 826 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Positive\rText: Quick and thorough answer by support agent Sonja!\rFrom several examples it comes that:\naccount word usually means a general problem with account pleo appears in formal reviews, mostly with some claim sonja seems to be a chat bot agent and it gets positive reviews Pretrained neural network Well, we got some new insights using custom model approach, let\u0026rsquo;s see whether the modern NN architecture will be able to unlock even more meaningful take-away\u0026rsquo;s without additional training\nCode\rfrom transformers import pipeline sentiment_transformer_model = pipeline( task=\u0026#34;sentiment-analysis\u0026#34;, model=\u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34;, return_all_scores=True ) scoring_results = sentiment_transformer_model(X_test[\u0026#34;lemma_text\u0026#34;].to_list()) The drawback of this approach becomes obvious as soon as you start applying it, even inference is taking a lot of time, fingers crossed it\u0026rsquo;s worths it, let\u0026rsquo;s look at the classification quality\nCode\rdef twitter_roberta_predict(scoring_output): scoring_output.sort(key=lambda x: x[\u0026#39;score\u0026#39;], reverse=True) prediction = scoring_output[scoring_output[0][\u0026#39;label\u0026#39;] == \u0026#39;neutral\u0026#39;][\u0026#39;label\u0026#39;] if prediction == \u0026#34;positive\u0026#34;: return 1 elif prediction == \u0026#34;negative\u0026#34;: return -1 else: raise ValueError(\u0026#34;unexpected scoring results\u0026#34;) sentiment_transformer_predictions = [twitter_roberta_predict(scoring) for scoring in scoring_results] print( classification_report(y_test, sentiment_transformer_predictions) ) precision recall f1-score support\r-1 0.19 0.23 0.21 111\r1 0.38 0.32 0.35 165\raccuracy 0.29 276\rmacro avg 0.29 0.28 0.28 276\rweighted avg 0.31 0.29 0.29 276\rIt\u0026rsquo;s kind of expected, given that we use the model which was trained and fine tuned on the texts which have a bit different nature (tweets). To unlock the true power of neural network approach it should be fine tuned to reflect the particular data specific and if you don\u0026rsquo;t have the sufficient amount of data - it should be a red flag not to wasting time with too comprehensive models\nAnyway, let\u0026rsquo;s take a look on how the language model works before pigeonholing it, here we are just some examples, to get a better summary a larger portion of the dataset is needed\nCode\rexplainer = shap.Explainer(sentiment_transformer_model) shap_values = explainer( X_train.loc[X_train[\u0026#34;lemma_text\u0026#34;].apply(lambda x: len(x.split()) \u0026gt; 5), \u0026#34;lemma_text\u0026#34;].sample(3), silent=True ) shap.plots.text(shap_values) ","permalink":"https://npodlozhniy.github.io/posts/feedback-analysis/","summary":"Background Every company has always taken pride in providing excellent customer service, therefore it\u0026rsquo;s crucial as part of ongoing improvements for product to collect and analyse feedback after each customer interaction on support channels\nA top-notch analyst in Hi-Tech industry should be in capable of handle at least base feedback analysis quickly and efficiently and going through this article you will be introduced with the necessary concepts and eventually given the base guide on how to unlock the true power of text data","title":"Customer Feedback Analysis in Python"},{"content":"Background We at HomeBuddy run a various number of AB tests to improve customer journey. A big part of efforts is allocated to onboarding funnel, hence the main metrics are conversions throughout this funnel. Usually we design multivariate tests with a few testing groups reflecting slightly different variations (often in terms of actual design) of the business hypothesis behind. No matter how you run the experiments you want to get an accurate and powerful procedure, that\u0026rsquo;s why we use Dunnett\u0026rsquo;s correction for all of the experiments where we have to maximize the power of AB engine. Are you curious what it is?\nPrerequisites It\u0026rsquo;s expected that the reader has an experience with Python and its main libraries for data analysis. The actual notebook was written on Python 3.11.4 and to keep the results reproducible here is the list of particular packages that are being used in the article specified with their versions.\nCode\rpip install --quiet --upgrade numpy==1.25.2 scipy==1.11.2 statsmodels==0.14.0 podlozhnyy_module==2.4b0 Problem Definition Imagine we want to optimize an onboarding funnel of an arbitrary product applying a new business idea. We don\u0026rsquo;t want to rely on expert assessment only and hence opt for designing an AB test first. The target metric is an abstract conversion and we carry out a classical AB test with one treatment group.\nIt\u0026rsquo;s not a secret that in such a scenario the best practice is a standard Z-test procedure for independent proportions\n$$ Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{P(1 - P)(\\frac{1}{n_1} + \\frac{1}{n_2})}} \\space\\text{,where}\\space P = \\frac{\\hat{p}_1n_1 + \\hat{p}_2n_2}{n_1 + n_2} $$\nUnder the conditions of the truth of the null hypothesis the statistic follows the standard normal distribution\n$$ Z \\stackrel{H_0}{\\sim} \\mathbb{N}(0, 1) $$\nCode\rimport numpy as np from scipy.stats import binom from statsmodels.stats.proportion import proportions_ztest, proportion_confint There are multiple versions of classical Z-test in Python and even though those have its unique selling points this time I apply my own implementation to stay consistent across this article using the same interface for all criteria.\nCode\rfrom podlozhnyy_module.abtesting import Dunnett, Ztest Interface As a brief check that the method is working as expected and to make you acquainted with its interface let\u0026rsquo;s identify a dummy experiment\ninput must consist of at least three lists:\nbase is the basis event that stands as a denominator of conversion target is the goal event - conversion numerator groups are names of the experiment variants In addition variant is specified to explicitly define which group we\u0026rsquo;re focus on (it makes sense in case of multivariate testing)\nCode\rinput = { \u0026#34;base\u0026#34;: [ 480, 437, ], \u0026#34;target\u0026#34;: [ 32, 37, ], \u0026#34;groups\u0026#34;: [ \u0026#34;test\u0026#34;, \u0026#34;control\u0026#34;, ], \u0026#34;variant\u0026#34;: \u0026#34;test\u0026#34;, } Functionality is written leveraging well-known OOP principles, so the experiment is not a function that stands aside, but a class which besides input dictionary gets the keys of the corresponding entities.\nCode\rtest = Ztest(input, base=\u0026#34;base\u0026#34;, target=\u0026#34;target\u0026#34;, groups=\u0026#34;groups\u0026#34;, variant=\u0026#34;variant\u0026#34;) print(f\u0026#34;p-value: {test.variant_pvalue(alternative=\u0026#39;less\u0026#39;):.3f}, Z-statistic: {test.statistic[0]:.3f}\u0026#34;) p-value: 0.151, Z-statistic: -1.032\rFor an on fly verification, the results might be compared to the output of a well known statsmodels package.\nCode\rcount = input[\u0026#34;target\u0026#34;] nobs = input[\u0026#34;base\u0026#34;] stat, pval = proportions_ztest(count, nobs, alternative=\u0026#39;smaller\u0026#39;) print(\u0026#39;p-value: {0:0.3f}, Z-statistic: {1:0.3f}\u0026#39;.format(pval, stat)) p-value: 0.151, Z-statistic: -1.032\rThe numbers coincide and now we\u0026rsquo;re ready to move to the main topic.\nTheory While often basic Z-test procedure is the appropriate option for AB analysis, it doesn\u0026rsquo;t satisfy the genuine requirements for the statistical method when it comes to multiple hypothesis testing. I hope you understand what is the problem in case of having $n$ independent metrics in your test: if you set up acceptable Type I error rate $\\alpha$ for each of them then the total probability to get at least one significant difference (dubbed as FWER) under the conditions of the truth of the null hypothesis (which means no difference between the groups by design) would be not $\\alpha$ but $1 - (1 - \\alpha)^n$ what totally invalidates the procedure.\nThere are two possible scenarios: either we have multiple metrics that we want to compare across the test and control group or we have several testing groups that apply different treatment to customers. Whilst the first problem is more popular by a wide margin and has lots of solutions, the second one is often neglected in the business industry and generally the same procedures are used to solve it. If you apply any type of corrections for multiple testing you\u0026rsquo;re already ahead of 80% of teams that don\u0026rsquo;t, although what I want to show you is that the second scenario must be treated differently to extract more insights from your experiments.\nDunnet\u0026rsquo;s test is a multiple comparison procedure developed specifically to compare each of a number of treatment groups to a control one, extended original paper was released in 1964. Dunnett\u0026rsquo;s test is not a set aside procedure but more like an extension of monumental Student\u0026rsquo;s T-test, for a specific design where every treatment group is compared to a single control one, which is leveraged to take into account the dependencies between comparisons. In case of proportions it\u0026rsquo;s a Z-test, as long as we don\u0026rsquo;t need to estimate variance for binomial distribution in addition to the probability of success.\nThe main trick is to calculate variance in a different way, just to remind you in case of a standard procedure statistic looks like\n$$ Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{S\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} $$\nwhere $S$ is a standard error estimate comprises the squared root of variance of a combined sample $\\sigma^2 = P(1 - P)$\nDunnett\u0026rsquo;s test statistic looks exactly the same for every treatment group with the only difference laying under how variance is calculated. In general in case of $n$ treatment groups and one control group $i=0$ of observations $(X_0, .. X_{N_i})$ with $N_i$ size of each, the formula is the following:\n$$ S^{2} = \\frac{\\sum_{i=0}^{n}S_i^2}{df} = \\frac{\\sum_{i=0}^{n}\\sum_{j=1}^{N_i}(X_{ij} - \\bar{X_i})^2}{df} = \\frac{\\sum_{i=0}^{n}\\sum_{j=1}^{N_i}X_{ij}^2 - n\\bar{X_i}^2}{df} $$\nwhere $df$ is degrees of freedom\n$$ df = \\sum_{i=0}^{n}N_{i} - (n + 1) $$\nFor proportions this \u0026ldquo;pooled\u0026rdquo; variance simplifies even further as long as the rest part of calculations is exactly the same\n$$ S^{2} = \\frac{\\sum_{i=0}^{n}{N_i\\bar{p_i}(1 - \\bar{p_i})}}{df} $$\nCanonical AB test First of all, to guarantee the accuracy we should challenge this specific criterion against a classical one in case when both of them are applicable - standard AB test. What is good about proportions is that we can easily simulate the data in the blink of an eye, so we\u0026rsquo;re setting up a simulation and employ Monte Carlo process to check two things:\ncorrectness - the criterion should meet the identified confidence level, which means that in case of AA comparison we should get Type I error in $\\alpha$ percent of experiments power - as it comes from the theory in case of only two groups Dunnett\u0026rsquo;s test is equal to a classical Z-test, so shall we call the implementation out? In addition to a point estimate of False Positive Rate I suggest building a 90% confidence interval to be more precise in the comparisons\nCode\rdef generate_sample(size: int, prob: float) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Return the number of target events \u0026#34;\u0026#34;\u0026#34; return binom(n=size, p=prob).rvs() Correctness in AA Code\rnp.random.seed(2024) alpha = 0.05 n_iterations = 1000 p_general = 0.1 input = dict.fromkeys([\u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;]) input[\u0026#34;names\u0026#34;] = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;] input[\u0026#34;variant\u0026#34;] = \u0026#34;B\u0026#34; for size in map(int, [1e2, 1e3, 1e4]): dunnett_positives = 0 z_positives = 0 for i in range(n_iterations): input[\u0026#34;base\u0026#34;] = [size, size] input[\u0026#34;target\u0026#34;] = [generate_sample(size, p_general), generate_sample(size, p_general)] dunnett_test = Dunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) z_test = Ztest(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) dunnett_p_value = dunnett_test.variant_pvalue(alternative=\u0026#34;two-sided\u0026#34;) z_p_value = z_test.variant_pvalue(alternative=\u0026#34;two-sided\u0026#34;) if dunnett_p_value \u0026lt;= alpha: dunnett_positives += 1 if z_p_value \u0026lt;= alpha: z_positives += 1 print(f\u0026#39;FPR for sample size {size}\u0026#39;) l, r = proportion_confint(count=dunnett_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#39;Dunnet: {dunnett_positives / n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#39;) l, r = proportion_confint(count=z_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#39;Z-test: {z_positives / n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#39;) print() FPR for sample size 100\rDunnet: 0.048 ¬± 0.011\rZ-test: 0.048 ¬± 0.011\rFPR for sample size 1000\rDunnet: 0.050 ¬± 0.011\rZ-test: 0.050 ¬± 0.011\rFPR for sample size 10000\rDunnet: 0.051 ¬± 0.011\rZ-test: 0.051 ¬± 0.011\rAmazing! It seems that 0.05 every time lies in the 90% confidence interval for FPR and hence the criterion is valid and moreover the numbers are exactly the same, it\u0026rsquo;s what was expected and now let\u0026rsquo;s check the power too.\nPower in AB Code\rnp.random.seed(2024) alpha = 0.05 n_iterations = 1000 p_general = 0.10 effect_size = 0.2 input = dict.fromkeys([\u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;]) input[\u0026#34;names\u0026#34;] = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;] input[\u0026#34;variant\u0026#34;] = \u0026#34;B\u0026#34; for size in map(int, [1e2, 1e3, 1e4]): dunnett_positives = 0 z_positives = 0 for i in range(n_iterations): input[\u0026#34;base\u0026#34;] = [size, size] input[\u0026#34;target\u0026#34;] = [generate_sample(size, p_general), generate_sample(size, p_general * (1 + effect_size))] dunnett_test = Dunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) z_test = Ztest(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) dunnett_p_value = dunnett_test.variant_pvalue(alternative=\u0026#34;two-sided\u0026#34;) z_p_value = z_test.variant_pvalue(alternative=\u0026#34;two-sided\u0026#34;) if dunnett_p_value \u0026lt;= alpha: dunnett_positives += 1 if z_p_value \u0026lt;= alpha: z_positives += 1 print(f\u0026#39;TPR of {effect_size:.0%} effect size for sample size {size}\u0026#39;) l, r = proportion_confint(count=dunnett_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#39;Dunnet: {dunnett_positives / n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#39;) l, r = proportion_confint(count=z_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) print(f\u0026#39;Z-test: {z_positives / n_iterations:.3f} ¬± {(r - l) / 2:.3f}\u0026#39;) print() TPR of 20% effect size for sample size 100\rDunnet: 0.085 ¬± 0.015\rZ-test: 0.085 ¬± 0.015\rTPR of 20% effect size for sample size 1000\rDunnet: 0.306 ¬± 0.024\rZ-test: 0.306 ¬± 0.024\rTPR of 20% effect size for sample size 10000\rDunnet: 0.992 ¬± 0.005\rZ-test: 0.992 ¬± 0.005\rWe are well on our way - the numbers are exactly the same, which means that in the case of 2 groups, Dunnett\u0026rsquo;s test is at least as powerful as a standard procedure. It\u0026rsquo;s time to challenge it in a way it\u0026rsquo;s supposed to be used: meet multivariate testing!\nMultivariate ABC Monte Carlo Now we will track how the criterion controls not a single FPR, but family-wise error rate (FWER) and what is more in order to continue the comparison with a classical Z-test the latter needs to have Bonferroni correction applied otherwise it wouldn\u0026rsquo;t properly control FWER.\nCode\rdef fwe(x: np.ndarray, alpha: float) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Indicates either at least one of null hypotheses is rejected \u0026#34;\u0026#34;\u0026#34; return max(x \u0026lt;= alpha) def bonferroni_fwe(x: np.ndarray, alpha: float, n: int) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Bonferroni correction for FWER, you can think of as it\u0026#39;s Bonferroni-Holm procedure Because to have a False Positive result it\u0026#39;s necessary and sufficient that at least one of `n` p-values doesn\u0026#39;t exceed `alpha` / `n` \u0026#34;\u0026#34;\u0026#34; return fwe(x, alpha / n) To simplify the code in the future Monte Carlo procedure is wrapped into a function with all the necessary parameters supplied as arguments.\nCode\rdef monte_carlo( aa_test: bool = True, verbose: bool = True, n_iterations: int = 1000, group_size: int = 100, n_groups: int = 3, p_general: float = 0.1, effect_size: float = 0.2, alpha: float = 0.05, ) -\u0026gt; dict: input = dict.fromkeys([\u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;]) input[\u0026#34;names\u0026#34;] = [chr(ord(\u0026#34;A\u0026#34;) + i) for i in range(n_groups)] input[\u0026#34;variant\u0026#34;] = input[\u0026#34;names\u0026#34;][-1] dunnett_positives = 0 z_positives = 0 for i in range(n_iterations): input[\u0026#34;base\u0026#34;] = [group_size] * n_groups input[\u0026#34;target\u0026#34;] = [generate_sample(group_size, p_general) for _ in range(n_groups - 1)] input[\u0026#34;target\u0026#34;] += [generate_sample(group_size, p_general * (1 + effect_size * (1 - aa_test)))] dunnett_test = Dunnett(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) z_test = Ztest(input, \u0026#34;base\u0026#34;, \u0026#34;target\u0026#34;, \u0026#34;names\u0026#34;, \u0026#34;variant\u0026#34;) dunnett_p_value = dunnett_test.groups_results(alternative=\u0026#34;two-sided\u0026#34;)[\u0026#34;p-value\u0026#34;] z_p_value = z_test.groups_results(alternative=\u0026#34;two-sided\u0026#34;)[\u0026#34;p-value\u0026#34;] if aa_test: if fwe(dunnett_p_value, alpha): dunnett_positives += 1 if bonferroni_fwe(z_p_value, alpha, 2): z_positives += 1 else: if isinstance(dunnett_p_value, np.ndarray) and dunnett_p_value[-1] \u0026lt;= alpha: dunnett_positives += 1 elif isinstance(dunnett_p_value, np.float64) and dunnett_p_value \u0026lt;= alpha: dunnett_positives += 1 if sidak_holm(z_p_value, alpha)[-1]: z_positives += 1 dl, dr = proportion_confint(count=dunnett_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) zl, zr = proportion_confint(count=z_positives, nobs=n_iterations, alpha=0.10, method=\u0026#39;wilson\u0026#39;) if verbose: print ( f\u0026#34;{\u0026#39;FPR\u0026#39; if aa_test else f\u0026#39;TPR of {effect_size:.0%} effect size\u0026#39;} for sample size {group_size}\\n\u0026#34; f\u0026#34; - Dunnett: {dunnett_positives / n_iterations:.3f} ¬± {(dr - dl) / 2:.3f}\\n\u0026#34; f\u0026#34; - Z-Test: {z_positives / n_iterations:.3f} ¬± {(zr - zl) / 2:.3f}\\n\u0026#34; ) return {\u0026#34;dunnett\u0026#34;: [dl, dunnett_positives / n_iterations, dr], \u0026#34;z-test\u0026#34;: [zl, z_positives / n_iterations, zr]} Correctness Validity first, let\u0026rsquo;s check the ability to control FWER at predefined $\\alpha$ level.\nCode\rnp.random.seed(2024) for size in [1e2, 5e2, 1e3, 5e3]: _ = monte_carlo(aa_test=True, group_size=int(size)) FPR for sample size 100\r- Dunnett: 0.045 ¬± 0.011\r- Z-Test: 0.042 ¬± 0.010\rFPR for sample size 500\r- Dunnett: 0.048 ¬± 0.011\r- Z-Test: 0.045 ¬± 0.011\rFPR for sample size 1000\r- Dunnett: 0.063 ¬± 0.013\r- Z-Test: 0.056 ¬± 0.012\rFPR for sample size 5000\r- Dunnett: 0.050 ¬± 0.011\r- Z-Test: 0.046 ¬± 0.011\rSuper cool, both methods: Dunnett\u0026rsquo;s Test without any corrections and Z-Test with Bonferroni-Holm correction control FWER correctly.\nPower Now it\u0026rsquo;s the time to define a full-fledged step-down procedure for multivariate testing. Despite the fact that its shortened version works well to define FWER it doesn\u0026rsquo;t when it comes to a power analysis. I prefer Sidak-Holm procedure as it\u0026rsquo;s known as the most powerful procedure that controls FWER, however as long as sample size is increased the difference from Bonferroni-Holm is hardly noticeable.\nCode\rdef sidak_holm(p_values: np.ndarray, alpha: float) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34; Step down Sidak-Holm procedure If the statistics are jointly independent, no procedure can be constructed to control FWER that is more powerful than the Sidak-Holm method \u0026#34;\u0026#34;\u0026#34; m = p_values.size adjusted_alpha = np.array([1 - (1 - alpha) ** (1 / (m - i + 1)) for i in range(1, m + 1)]) sorted_indexes = np.argsort(p_values) sorted_pvalues = p_values[sorted_indexes] first_reject = (list(sorted_pvalues \u0026lt;= adjusted_alpha) + [False]).index(False) result = np.array([True] * first_reject + [False] * (m - first_reject)) return result[np.argsort(sorted_indexes)] For the power test I offer to use two treatment groups and a single control where in one of the treatments an effect of 20% uplift is added. So the null hypothesis should be rejected and True Positive Rate is measured - is the share of rejected hypotheses among the number of iterations.\nCode\rnp.random.seed(2024) for size in [1e2, 5e2, 1e3, 5e3]: _ = monte_carlo(aa_test=False, group_size=int(size)) TPR of 20% effect size for sample size 100\r- Dunnett: 0.051 ¬± 0.011\r- Z-Test: 0.043 ¬± 0.011\rTPR of 20% effect size for sample size 500\r- Dunnett: 0.104 ¬± 0.016\r- Z-Test: 0.092 ¬± 0.015\rTPR of 20% effect size for sample size 1000\r- Dunnett: 0.265 ¬± 0.023\r- Z-Test: 0.240 ¬± 0.022\rTPR of 20% effect size for sample size 5000\r- Dunnett: 0.855 ¬± 0.018\r- Z-Test: 0.842 ¬± 0.019\rThe results are promising! The power of Dunnett\u0026rsquo;s test every time exceeds the power of Z-test with Sidak-Holm procedure applied. The difference is not significant though, so we can\u0026rsquo;t say for sure that it\u0026rsquo;s better, let\u0026rsquo;s experiment more with parameters and change effect_size\nCode\rnp.random.seed(2024) for effect_size in [0.1, 0.2, 0.3]: _ = monte_carlo(aa_test=False, group_size=3000, effect_size=effect_size) TPR of 10% effect size for sample size 3000\r- Dunnett: 0.163 ¬± 0.019\r- Z-Test: 0.157 ¬± 0.019\rTPR of 20% effect size for sample size 3000\r- Dunnett: 0.589 ¬± 0.026\r- Z-Test: 0.569 ¬± 0.026\rTPR of 30% effect size for sample size 3000\r- Dunnett: 0.927 ¬± 0.014\r- Z-Test: 0.914 ¬± 0.015\rIt\u0026rsquo;s exciting, the result remains the same, and if we know that Sidak-Holm is the most powerful method that controls FWER for the general use case, we see now that Dunnett\u0026rsquo;s at least not worse. Finally, the most appealing variable is the number of treatment groups, let\u0026rsquo;s vary it too.\nCode\rnp.random.seed(2024) for n_groups in [3, 5, 7]: _ = monte_carlo(aa_test=False, group_size=3000, n_groups=n_groups) TPR of 20% effect size for sample size 3000\r- Dunnett: 0.608 ¬± 0.025\r- Z-Test: 0.601 ¬± 0.025\rTPR of 20% effect size for sample size 3000\r- Dunnett: 0.544 ¬± 0.026\r- Z-Test: 0.506 ¬± 0.026\rTPR of 20% effect size for sample size 3000\r- Dunnett: 0.464 ¬± 0.026\r- Z-Test: 0.424 ¬± 0.026\rNow we\u0026rsquo;ve nailed it! The number of groups is what affects the bottom line. The more groups are in the experiment - the more powerful Dunnett\u0026rsquo;s Correction than Sidak-Holm. So, let\u0026rsquo;s build a title image for this article that illustrates how Dunnett\u0026rsquo;s test outperforms the well-known step-down procedure as the number of treatment groups increases.\nCode\rfrom tqdm.notebook import tqdm np.random.seed(2024) ztest_values = [] dunnett_values = [] for n_groups in tqdm(range(2, 16)): result = monte_carlo(aa_test=False, verbose=False, group_size=3000, n_groups=n_groups) dunnett_values.append(result[\u0026#34;dunnett\u0026#34;]) ztest_values.append(result[\u0026#34;z-test\u0026#34;]) Plotly is used for interactive visualization, hover over the image to see details.\nCode\rimport plotly.express as px import plotly.graph_objs as go def hex2rgba(hex, alpha): \u0026#34;\u0026#34;\u0026#34; Convert plotly hex colors to rgb and enables transparency adjustment \u0026#34;\u0026#34;\u0026#34; col_hex = hex.lstrip(\u0026#39;#\u0026#39;) col_rgb = tuple(int(col_hex[i : i + 2], 16) for i in (0, 2, 4)) col_rgb += (alpha,) return \u0026#39;rgba\u0026#39; + str(col_rgb) def get_new_color(colors): while True: for color in colors: yield color colors_list = px.colors.qualitative.Plotly rgba_colors = [hex2rgba(color, alpha=0.5) for color in colors_list] palette = get_new_color(rgba_colors) def add_chart(figure, data, title): x = list(range(1, 15)) color = next(palette) figure.add_trace( go.Scatter( name=title, x=x, y=[v[1] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(color=color), ), ) figure.add_trace( go.Scatter( name=\u0026#39;Upper Bound\u0026#39;, x=x, y=[v[2] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(width=0), marker=dict(color=\u0026#34;#444\u0026#34;), hovertemplate=\u0026#34;%{y:.3f}\u0026#34;, showlegend=False, ), ) figure.add_trace( go.Scatter( name=\u0026#39;Lower Bound\u0026#39;, x=x, y=[v[0] for v in data], mode=\u0026#39;lines\u0026#39;, line=dict(width=0), marker=dict(color=\u0026#34;#444\u0026#34;), hovertemplate=\u0026#34;%{y:.3f}\u0026#34;, showlegend=False, fillcolor=color, fill=\u0026#39;tonexty\u0026#39;, ), ) figure = go.Figure() add_chart(figure, dunnett_values, \u0026#34;Dunnnett\u0026#39;s Correction\u0026#34;) add_chart(figure, ztest_values, \u0026#39;Z-Test with Sidak-Holm\u0026#39;) figure.update_xaxes( title_text=\u0026#34;Number of Treatment Groups\u0026#34; ) figure.update_layout( yaxis_title=\u0026#39;True Positive Rate\u0026#39;, title={ \u0026#34;x\u0026#34;: 0.5, \u0026#34;text\u0026#34;: \u0026#39;Power of –°riterion\u0026#39;, }, hovermode=\u0026#34;x\u0026#34;, template=\u0026#34;plotly_dark\u0026#34;, ) figure.show() Conclusion It was shown that when the experiment design satisfies the premises of Dunnett\u0026rsquo;s Test applicability (only $n$ comparisons of $n$ test groups against a single control) at least in a specific case of conversion metrics, Dunnett\u0026rsquo;s correction is more powerful that the standard step-down procedures like Sidak-Holm.\nWhile Dunnet\u0026rsquo;s correction is a definite winner it doesn\u0026rsquo;t mean that Sidak-Holm is abandoned from now on in our team, the proper design would be to use Dunnett\u0026rsquo;s correction first for multivariate testing and Sidak-Holm procedure must be applied on top if there are multiple metrics to compare between the groups, which is often the case.\nAdditional Information Original Paper from Journal of the American Statistical Association Multivariate Testing - Best Practices Dunnett\u0026rsquo;s Correction in Analytics ToolKit Multiple comparisons problem ","permalink":"https://npodlozhniy.github.io/posts/dunnett-correction/","summary":"Background We at HomeBuddy run a various number of AB tests to improve customer journey. A big part of efforts is allocated to onboarding funnel, hence the main metrics are conversions throughout this funnel. Usually we design multivariate tests with a few testing groups reflecting slightly different variations (often in terms of actual design) of the business hypothesis behind. No matter how you run the experiments you want to get an accurate and powerful procedure, that\u0026rsquo;s why we use Dunnett\u0026rsquo;s correction for all of the experiments where we have to maximize the power of AB engine.","title":"Dunnett's Correction for ABC testing"}]