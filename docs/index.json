[{"content":"\rBackground Every company has always taken pride in providing excellent customer service, therefore it\u0026rsquo;s crucial as part of ongoing improvements for product to collect and analyse feedback after each customer interaction on support channels\nA top-notch analyst in Hi-Tech industry should be in capable of handle at least base feedback analysis quickly and efficiently and going through this article you will be introduced with the necessary concepts and eventually given the base guide on how to unlock the true power of text data\nWell, no more words, let\u0026rsquo;s have a look on how\nDisclaimer: I\u0026rsquo;m not pretending on having this article as an exhaustive everything you need notebook for analyzing customer feedback, but it\u0026rsquo;s the steps I usually follow while working on customers text data and on the basis of my previous experience - 80% of your needs is covered here\nPrerequisites It\u0026rsquo;s expected that the reader has an experience with Python and it\u0026rsquo;s main Data Analysis libraries\nThe actual notebook was written on Python 3.7.9 and to keep the results reproducible here is the list of particular packages that are being used in the article specified with their versions\nCode\rnumpy==1.21.6 pandas==1.3.5 pandas-profiling==3.1.0 matplotlib==3.4.2 unidecode==1.3.7 nltk==3.6.2 sklearn==0.24.2 wordcloud==1.9.2 shap==0.42.1 transformers==4.30.2 gensim==4.0.1 Methodology During this notebook the feedback which comes in the form of ratings (from 1-5) and textual comments is considered. The general purpose is to dive deeper into this feedback, identify common themes, if certain issues lead to more negative feedback than others and understand areas of improvement\nCode\rimport numpy as np import pandas as pd import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) from matplotlib import pyplot as plt %matplotlib inline As a general rule the analysis should be organized in a top-down manner, simple exploration and low-hanging fruits first and sophisticated approach after only if you really need more in-depth expertise. According to the above the analysis will be organized in the three steps:\nData Mining - simple unsupervised data exploration to have a general idea of the data nature Sentiment Analysis - search on what matters the most which reasons drive review to be positive or negative Topic Modelling - main feedback themes extraction, improvement focuses identification Analysis Process Data Mining Basic EDA First of all - let\u0026rsquo;s have a fluent look at the suggested data, pandas-profiling is a very useful tool to perform basic and boring Exploratory Data Analysis in a minute. View Data Profile by clicking here\nCode\rfrom pandas_profiling import ProfileReport df = pd.read_csv(\u0026#39;feedback-data-sample.csv\u0026#39;, index_col=0) profile = ProfileReport( df, minimal=True, dark_mode=True, title=\u0026#34;Feedback Data Report\u0026#34;, ) profile.to_file(\u0026#34;data-profile.html\u0026#34;) Well, what are the main data patterns\nthe dataset constitutes 1276 tickets with customer feedback consisted of csat_score and comment ticket_id - is a primary key, given that we don\u0026rsquo;t have timestamp column and it\u0026rsquo;s looks like bigint not a random hash, it\u0026rsquo;s better to sort by it beforehand, to be sure we are not predicting the past on the future features all the reviews are in English language, good for us, we can forget about additional translators and this column in general csat_score has only two different values 0 and 4 stars, and the majority of reviews are positive, well not much, but it\u0026rsquo;s even easier to transform it into a bool target and work with it further comment has a few NULL values, let\u0026rsquo;s keep it in mind from the simple frequentist analysis it\u0026rsquo;s clear that tokens not and very might be informative, so don\u0026rsquo;t forget to exclude them from stop words list Code\rdf = df.sort_index().drop(columns={\u0026#39;language\u0026#39;}) def define_sentiment(rating: float) -\u0026gt; int: if rating \u0026lt; 3: return -1 # negative sentiment elif rating \u0026gt; 3: return 1 # positive sentiment else: return 0 # neutral sentiment df[\u0026#39;sentiment\u0026#39;] = df[\u0026#39;csat_score\u0026#39;].apply(define_sentiment) Text cleaning Barely the role of data cleaning might be underestimated, it\u0026rsquo;s an incredibly important step, if this is skipped the rest of analysis doesn\u0026rsquo;t make any sense then.\nMain cleaning that should be applied:\nremove all the symbols and keep only words remove redundant short words remove general language words transliterate unicode symbols to ascii lowercase The next step is tokenization: the are two main approaches here:\nstemming - fast process of removing prefixes and suffixes to give a word a short form, that might not be a dictionary word though lemmatization - finds meaningful word representation from dictionary and depends on the part-of-speech To summarize, the stemming is just searching for a common ground between words and cutting ends then and therefore it takes less time whereas lemmatization provides better results by performing a specific morphological analysis and produces a real word which is extremely important for some human-interactive applications\nSounds like if the resources are not a problem it\u0026rsquo;s better to use lemmatization by default, but there is an opinion that Stemming works efficiently for some specific tasks like: spam classification and feedback sentiment classification, given that it\u0026rsquo;s the case, let\u0026rsquo;s apply both and take a choice in the end\nCode\rimport re import nltk # If the code below doesn\u0026#39;t work - download add-ons first # nltk.download([\u0026#39;stopwords\u0026#39;, \u0026#39;wordnet\u0026#39;]) from nltk.corpus import stopwords stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) # this words might be useful, better to retain for now for word in [\u0026#39;very\u0026#39;, \u0026#39;not\u0026#39;]: stop_words.remove(word) from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() # better version of the Porter Stemmer from nltk.stem.snowball import SnowballStemmer stemmer = SnowballStemmer(language=\u0026#34;english\u0026#34;) for example in [\u0026#39;programming\u0026#39;, \u0026#39;quickly\u0026#39;, \u0026#39;very\u0026#39;]: print(f\u0026#34;Example: {example}\u0026#34;) print(f\u0026#34; - Lemma: {lemmatizer.lemmatize(example, pos=\u0026#39;v\u0026#39;)}\u0026#34;) print(f\u0026#34; - Stem: {stemmer.stem(example)}\u0026#34;) Example: programming\r- Lemma: program\r- Stem: program\rExample: quickly\r- Lemma: quickly\r- Stem: quick\rExample: very\r- Lemma: very\r- Stem: veri\rUnicode transliteration is needed, given that the data contains non-ascii symbols\nCode\rfrom unidecode import unidecode df[df.comments.notnull()][ df[df.comments.notnull()].comments != df[df.comments.notnull()].comments.apply(unidecode) ].comments.sample(5) ticket_id\r43532202076873 Just asking me to show what I‚Äôm doing to give ...\r43532202076331 I haven‚Äôt had a reply\r43532202117219 All good üëç thanks\r43532202073174 Chat went silent. After talking to someone the...\r43532202202546 Hi Val√©ria,\\nIt still does not work. There mus...\rName: comments, dtype: object\rPutting it all together\nCode\rdef clean_data(x, tokenizer, black_list=stop_words): \u0026#34;\u0026#34;\u0026#34; The method removes from a sentence `x` - punctuation \u0026amp; digits - too short words (less than 3 letters) - unicode symbols (translate to ascii) - words from `black_list` Return lowercased and tokenized text \u0026#34;\u0026#34;\u0026#34; words = re.findall(\u0026#39;\\w{3,}\u0026#39;, re.sub(\u0026#39;[^a-z√Ä-√ø ]\u0026#39;, \u0026#39; \u0026#39;, str(x).lower() if x is not np.NaN else \u0026#39;\u0026#39;)) tokens = [tokenize(unidecode(word), tokenizer) for word in words] return \u0026#39; \u0026#39;.join([word for word in tokens if word not in black_list]) def tokenize(x: str, tokenizer) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Applies either stemming or lemmatization to a token `x` \u0026#34;\u0026#34;\u0026#34; if hasattr(tokenizer, \u0026#39;lemmatize\u0026#39;): return tokenizer.lemmatize(x, pos=\u0026#39;v\u0026#39;) elif hasattr(tokenizer, \u0026#39;stem\u0026#39;): return tokenizer.stem(x) else: raise ValueError(\u0026#34;tokenizer should be either Lemmatizer or Stemmer\u0026#34;) df[\u0026#34;lemma_text\u0026#34;] = df.comments.apply(clean_data, args=(lemmatizer,)) df[\u0026#34;stem_text\u0026#34;] = df.comments.apply(clean_data, args=(stemmer,)) Words Frequency There are many different ways how to tackle the visual text analysis, the popular and convenient way is Word Clouds where the size of the word reflects its frequency within the given text\nP.S. In any data mining initiative, it is a good idea to retain some portion of the data to validate your final findings, so let\u0026rsquo;s create a holdout piece of data to adhere true-to-life approach\nCode\rfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( df[[\u0026#34;lemma_text\u0026#34;, \u0026#34;stem_text\u0026#34;]], df[\u0026#39;sentiment\u0026#39;], train_size=1000, shuffle=False ) from collections import Counter def create_ngrams(tokens: list, n: int): ngrams = zip(*[tokens[idx:] for idx in range(n)]) return [\u0026#34; \u0026#34;.join(sorted(ngram)) for ngram in ngrams] def frequent_ngrams(documents: list, n: int = 1): \u0026#34;\u0026#34;\u0026#34; Use .most_common(top_n) to get top n ngrams \u0026#34;\u0026#34;\u0026#34; ngrams = [] if n == 1: ngrams = \u0026#34; \u0026#34;.join(list(documents)).split() elif n \u0026gt;= 2: for tokens in documents: ngrams.extend(create_ngrams(tokens.split(), n)) else: raise ValueError(\u0026#34;n for n-grams should be a positive number\u0026#34;) return Counter(ngrams) import wordcloud def make_word_cloud(text, stop_words=None): plt.figure(figsize=(12, 9)) kwargs = { \u0026#39;width\u0026#39;: 1600, \u0026#39;height\u0026#39;: 900, \u0026#39;min_font_size\u0026#39;: 10 } if isinstance(text, str): word_cloud = wordcloud.WordCloud(stopwords=stop_words, **kwargs).generate_from_text(text) elif isinstance(text, list) or isinstance(text, np.ndarray): word_cloud = wordcloud.WordCloud(stopwords=stop_words, **kwargs).generate(\u0026#34; \u0026#34;.join(text)) else: if stop_words: text = {word: value for word, value in text.items() if word not in stop_words} word_cloud = wordcloud.WordCloud(**kwargs).generate_from_frequencies(text) plt.imshow(word_cloud) plt.axis(\u0026#34;off\u0026#34;) plt.show() First of all, as promised different tokenizers should be compared\nCode\rmake_word_cloud(frequent_ngrams(X_train[\u0026#34;stem_text\u0026#34;], 1)) Code\rmake_word_cloud(frequent_ngrams(X_train[\u0026#34;lemma_text\u0026#34;], 1)) Well, at first glance it looks like both tokenizers work very similarly, one noticeable difference is that stemmer treats word pairs like help and helpful or quick and quickly as one token and actually it might be wrong, imagine if we encounter helpful more in positive sentence and help in negative, then they shouldn\u0026rsquo;t be united\nCode\rfor word in [\u0026#34;help\u0026#34;, \u0026#34;helpful\u0026#34;]: score = df.loc[ df[\u0026#34;comments\u0026#34;].apply( lambda x: f\u0026#34; {word} \u0026#34; in x if x is not np.nan else False ), \u0026#34;sentiment\u0026#34; ].mean() print(f\u0026#34;for `{word}` average sentiment score = {score:.2f}\u0026#34;) for `help` average sentiment score = 0.31\rfor `helpful` average sentiment score = 0.95\rIndeed, that is the case, so let\u0026rsquo;s end up with traditional lemmatizer and take a look at word clouds separately for different sentiments\nCode\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026lt; 0, \u0026#34;lemma_text\u0026#34;], 1)) Code\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026gt; 0, \u0026#34;lemma_text\u0026#34;], 1)) Good news, they are very different in essence, in positive reviews customers use gratitude words more like thank and helpful on the other hand in negative sentences customer highlight that the issue still not resolved. In addition it might be useful to take a look at popular collocations\nCode\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026gt; 0, \u0026#34;lemma_text\u0026#34;], 2)) Code\rmake_word_cloud(frequent_ngrams(X_train.loc[y_train \u0026lt; 0, \u0026#34;lemma_text\u0026#34;], 3)) N-grams appear to be very informative:\nin positive sentences, based on bigrams, customers say that the response was quick, problem was solved and the support was very helpful in negative sentences, on the basis of trigrams, customers claim that the issue/problem not resolved sometimes the add yet or still to fully express their dissatisfaction To summarize, the basic approach has already given some meaningful insights and the hope that the reviews might be classified automatically quite well and themes can be modelled then\nUnsupervised TF-IDF It\u0026rsquo;s critical to reduce feature number otherwise X-matrix will be too sparse, and the clusterization will fail to give a substantial result, actually only quite often words should be taken into account\nCode\rfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score from sklearn.cluster import KMeans min_word_counts = range(1, 100, 2) n_features, scores = [], [] for min_word_count in min_word_counts: tfidf = TfidfVectorizer(min_df=min_word_count) X = tfidf.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) n_features.append(X.shape[1]) km = KMeans( n_clusters=2, init=\u0026#39;k-means++\u0026#39;, max_iter=300, random_state=20231020 ) km.fit(X) score = accuracy_score(2 * km.predict(X) - 1, y_train) scores.append( max(score, 1 - score) ) plt.style.use(\u0026#39;ggplot\u0026#39;) fig, ax1 = plt.subplots(figsize=(12, 6)) ax2 = ax1.twinx() ax2.bar(min_word_counts, n_features, color=\u0026#34;b\u0026#34;, width=1.5, alpha=0.33) ax1.plot(min_word_counts, scores, \u0026#34;g--\u0026#34;, linewidth=2) ax1.set_ylabel(\u0026#39;Accuracy, %\u0026#39;, color=\u0026#34;g\u0026#34;) ax1.set_xlabel(\u0026#39;Minimal Word Frequency, #\u0026#39;) ax2.set_ylabel(\u0026#39;N Features, #\u0026#39;, color=\u0026#34;b\u0026#34;) plt.title(\u0026#39;K-Means Clusterization\u0026#39;) plt.show() From the chart is clear that the larger number of features doesn\u0026rsquo;t lead to the Accuracy increase, from the principal of maximum Accuracy the optimal number of features might be 69 for example, let\u0026rsquo;s fix it and track which features the model decided to consider\nCode\rtfidf = TfidfVectorizer(min_df=69) X = tfidf.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) words = np.array(tfidf.get_feature_names()) print(f\u0026#34;Number of features: {X.shape[1]}, namely: \u0026#34;) print(*words) Number of features: 14, namely: answer get help helpful issue not problem quick resolve response solve still thank very\rThe clusterization which is based just on 14 words! gives an accuracy higher than 75%, but it\u0026rsquo;s the result only valid for the train sample, which was used to identify min_df hyperparameter, so to have an unbiased estimation test sample should be considered here\nCode\rkm = KMeans( n_clusters=2, init=\u0026#39;k-means++\u0026#39;, max_iter=300, random_state=20231020 ) km.fit(X) centroids_important_indexes = km.cluster_centers_.argsort()[:,::-1] for idx in range(km.get_params()[\u0026#39;n_clusters\u0026#39;]): print(\u0026#34;Cluster No.\u0026#34;, idx, *words[centroids_important_indexes[idx, :7]]) Cluster No. 0 very issue thank helpful quick resolve response\rCluster No. 1 not issue resolve still solve problem get\rWell, looks like Cluster-0 caught positive feedback and Cluster-1 negative, then given that target is the value from [-1, 1] set, to define an accuracy some transformation must be put in place first\nCode\rpredictions_train = 2 * -km.predict(X) + 1 print(f\u0026#34;Accuracy of K-Means train sample = {accuracy_score(predictions_train, y_train):.1%}\u0026#34;) Accuracy of K-Means train sample = 76.4%\rCode\rpredictions_test = 2 * -km.predict( tfidf.transform( X_test[\u0026#34;lemma_text\u0026#34;] ) ) + 1 print(f\u0026#34;Accuracy of K-Means test sample = {accuracy_score(predictions_test, y_test):.1%}\u0026#34;) Accuracy of K-Means test sample = 74.6%\rSplendid, this toy example gives a clue that it\u0026rsquo;s pretty good approach when you need to classify your customer\u0026rsquo;s feedback while you don\u0026rsquo;t have any ratings (only text comments). Unsupervised approach based on TF-IDF Vectorizer and K-Means gives a nice baseline, but fortunately, it\u0026rsquo;s not our case let\u0026rsquo;s go ahead and use rating set by a customer in addition to texts to reach the text data potential\nSentiment Analysis The goal of this part of the article is to enhance the unsupervised model and build a powerful classifier to eventually understand key drivers for review to be positive or negative from features extraction\nThere are 2 main ways of doing Semantic Analysis:\ntrain your own model using the available data use pre-trained deep learning models and fine-tune them if needed for this particular text specific Both approaches is considered below\nCustom Regression Model Training of the custom model will be held in 3 steps (again?)\nModel architecture selection Selected model training and cross-validation Feature analysis and general evaluation Well, by the model term stands combination of Vectorizer, which transform text data into a vector representation and Classifier that is training on these vectors to predict sentiment\nOut of vectorizers we are going to try both most popular options: Classic Counter and TF-iDF, for classificators let\u0026rsquo;s search among classic linear, tree-based and in addition naive bias method, which might be extremely useful for text classification\nIn addition as it was shown during unsupervised analysis, barely all the words should be taken into account to build a substantial model and it alleviates the learning process also, therefore SVD application for feature space reduction will be considered\nCode\rfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import MultinomialNB from sklearn.svm import LinearSVC from sklearn.decomposition import TruncatedSVD from sklearn.model_selection import cross_val_score from sklearn.pipeline import Pipeline from typing import List, Union, Optional def model(vectorizer, classifier, transformer=None): if transformer: return Pipeline([ (\u0026#34;vectorizer\u0026#34;, vectorizer), (\u0026#34;transformer\u0026#34;, transformer), (\u0026#34;classifier\u0026#34;, classifier) ]) else: return Pipeline([ (\u0026#34;vectorizer\u0026#34;, vectorizer), (\u0026#34;classifier\u0026#34;, classifier) ]) def get_entity_name(entity: Union[List[object], Optional[object]]) -\u0026gt; str: if not isinstance(entity, List): entity = [entity] return [re.sub(\u0026#34;\u0026gt;|\u0026#39;\u0026#34;, \u0026#39;\u0026#39;, str(e)).split(\u0026#34;.\u0026#34;)[-1] for e in entity if e] Using vanilla classifiers let\u0026rsquo;s define which one suits the data better from scratch and reveal hyperparameters on the validation basis after for this candidate\nCode\rnp.random.seed(20231024) for vectorizer in [CountVectorizer, TfidfVectorizer]: for classifier in [LogisticRegression, SGDClassifier, RandomForestClassifier, LinearSVC, MultinomialNB]: transformers = [None] if vectorizer == CountVectorizer: transformers.append(TfidfTransformer()) if classifier != MultinomialNB: transformers.extend([ TruncatedSVD(n_components=100), TruncatedSVD(n_components=10), ]) for transformer in transformers: print(get_entity_name([vectorizer, classifier, transformer]), end=\u0026#34;: \u0026#34;) score = cross_val_score( model(vectorizer(), classifier(), transformer), X_train[\u0026#34;lemma_text\u0026#34;], y_train, cv=5, scoring=\u0026#39;f1\u0026#39; ).mean() print(f\u0026#34;f1-score: {score:.1%}\u0026#34;) ['CountVectorizer', 'LogisticRegression']: f1-score: 90.2%\r['CountVectorizer', 'LogisticRegression', 'TfidfTransformer()']: f1-score: 90.5%\r['CountVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=100)']: f1-score: 88.7%\r['CountVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=10)']: f1-score: 87.5%\r['CountVectorizer', 'SGDClassifier']: f1-score: 88.0%\r['CountVectorizer', 'SGDClassifier', 'TfidfTransformer()']: f1-score: 89.5%\r['CountVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.0%\r['CountVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 84.7%\r['CountVectorizer', 'RandomForestClassifier']: f1-score: 90.2%\r['CountVectorizer', 'RandomForestClassifier', 'TfidfTransformer()']: f1-score: 90.1%\r['CountVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 89.1%\r['CountVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 88.3%\r['CountVectorizer', 'LinearSVC']: f1-score: 89.3%\r['CountVectorizer', 'LinearSVC', 'TfidfTransformer()']: f1-score: 90.3%\r['CountVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=100)']: f1-score: 89.2%\r['CountVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=10)']: f1-score: 87.3%\r['CountVectorizer', 'MultinomialNB']: f1-score: 90.6%\r['CountVectorizer', 'MultinomialNB', 'TfidfTransformer()']: f1-score: 90.2%\r['TfidfVectorizer', 'LogisticRegression']: f1-score: 90.5%\r['TfidfVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=100)']: f1-score: 89.7%\r['TfidfVectorizer', 'LogisticRegression', 'TruncatedSVD(n_components=10)']: f1-score: 86.7%\r['TfidfVectorizer', 'SGDClassifier']: f1-score: 89.4%\r['TfidfVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.4%\r['TfidfVectorizer', 'SGDClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 83.5%\r['TfidfVectorizer', 'RandomForestClassifier']: f1-score: 89.9%\r['TfidfVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=100)']: f1-score: 88.9%\r['TfidfVectorizer', 'RandomForestClassifier', 'TruncatedSVD(n_components=10)']: f1-score: 88.0%\r['TfidfVectorizer', 'LinearSVC']: f1-score: 90.3%\r['TfidfVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=100)']: f1-score: 90.7%\r['TfidfVectorizer', 'LinearSVC', 'TruncatedSVD(n_components=10)']: f1-score: 87.4%\r['TfidfVectorizer', 'MultinomialNB']: f1-score: 90.2%\rWinners:\nTfidfVectorizer \u0026amp; LinearSVC \u0026amp; TruncatedSVD CountVectorizer \u0026amp; MultinomialNB TfidfVectorizer \u0026amp; LogisticRegression On the basis of above analysis, the better approach will be to go with the first option because:\nthere is a clear rationale to apply tf-idf over usual counter for the majority of text analysis task regression is more flexible model than Naive Bayes and highly likely that after cross-validation it can accomplish even higher accuracy reducing the feature space makes sense as it was for unsupervised learning Code\rfrom sklearn.model_selection import GridSearchCV clf = model( vectorizer=TfidfVectorizer(), classifier=LinearSVC(random_state=20231020), transformer=TruncatedSVD() ) param_grid = { \u0026#34;vectorizer__max_df\u0026#34;: [1.0, 0.15, 0.10], \u0026#34;vectorizer__min_df\u0026#34;: [1, 2, 3], \u0026#34;vectorizer__ngram_range\u0026#34;: [(1, 1), (1, 2), (1, 3)], \u0026#34;classifier__C\u0026#34;: [0.75, 1, 1.25], \u0026#34;transformer__n_components\u0026#34;: [100, 500, 1000] } search = GridSearchCV(clf, param_grid, cv=3) search.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(\u0026#34;Best parameter (CV score = %0.3f):\u0026#34; % search.best_score_) for key, value in search.best_params_.items(): print(f\u0026#34;{key}: {value}\u0026#34;) Best parameter (CV score = 0.897):\rclassifier__C: 1\rclassifier__loss: hinge\rtransformer__n_components: 1000\rvectorizer__max_df: 1.0\rvectorizer__min_df: 1\rvectorizer__ngram_range: (1, 2)\rCode\r# If you want to see all the results of the scoring use the following code # for param, score in zip( # search.cv_results_[\u0026#39;params\u0026#39;], # search.cv_results_[\u0026#39;mean_test_score\u0026#39;] # ): # print(param, score) The majority of parameters remains as by default, although some of them changed, it\u0026rsquo;s interesting that only bigrams are useful, trigrams on contrary to what we saw before from dummy analysis don\u0026rsquo;t give any additional info to regression from grid-search point of view, from given experience it\u0026rsquo;s better to retain them\nWell, given the parameters are all defined, let\u0026rsquo;s set them up and take closer look at the trained model\nCode\rfrom sklearn.metrics import classification_report clf = model( vectorizer=TfidfVectorizer(ngram_range=(1, 3)), classifier=LinearSVC(random_state=20231020), transformer=TruncatedSVD(n_components=1000), ) clf.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(classification_report(y_test, clf.predict(X_test[\u0026#34;lemma_text\u0026#34;]))) precision recall f1-score support\r-1 0.90 0.86 0.88 111\r1 0.91 0.94 0.92 165\raccuracy 0.91 276\rmacro avg 0.91 0.90 0.90 276\rweighted avg 0.91 0.91 0.91 276\rIf we just out of curiosity take a look at another candidate - Naive Bayes, then the results are\nCode\rclf = model( vectorizer=CountVectorizer(ngram_range=(1, 3)), classifier=MultinomialNB(), ) clf.fit(X_train[\u0026#34;lemma_text\u0026#34;], y_train) print(classification_report(y_test, clf.predict(X_test[\u0026#34;lemma_text\u0026#34;]))) precision recall f1-score support\r-1 0.90 0.82 0.86 111\r1 0.89 0.94 0.91 165\raccuracy 0.89 276\rmacro avg 0.89 0.88 0.89 276\rweighted avg 0.89 0.89 0.89 276\rWell, regression works a bit better and it\u0026rsquo;s a pretty powerful classificator, but what is really needed is feature exploration, which words have been determined by the model sentiment and what do customers really appreciate or complain about. In order to evaluate features easily SVD step is skipped here, it doesn\u0026rsquo;t inflict tangible damage on model quality but simplifies analysis a lot\nCode\rimport shap vectorizer=TfidfVectorizer(ngram_range=(1, 3)) classifier=LinearSVC(random_state=20231020) X_train_vec = vectorizer.fit_transform(X_train[\u0026#34;lemma_text\u0026#34;]) classifier.fit(X_train_vec, y_train) explainer = shap.Explainer( classifier, X_train_vec, feature_names=vectorizer.get_feature_names() ) shap_values = explainer(X_train_vec) shap.plots.beeswarm(shap_values, max_display=30, plot_size=(12, 8)) Well, the results almost don\u0026rsquo;t reveal any new pattens:\nclients like quick and clear answered questions, they are thankful for fast and friendly support and of course solved problem is everything clients dislike: if have no response (reply) and if the problem still not resolved (yet), in general they don\u0026rsquo;t like to wait for getting a solution However, there are some new words here - account, pleo and sonja, and there is an easy way to check how the model works for a particular review\nP.S. if visualization doesn\u0026rsquo;t work run shap.initjs() first\nCode\rfor word in[\u0026#39;account\u0026#39;, \u0026#39;pleo\u0026#39;, \u0026#39;sonja\u0026#39;]: print(word, *X_train[\u0026#34;lemma_text\u0026#34;].apply(lambda x: word in x).values.argsort()[-3:]) account 154 375 104\rpleo 386 532 242\rsonja 826 943 190\rCode\ridx = 154 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Negative\rText: my email account is still not connected and it does not work. tried a couple of times to reconnect\rCode\ridx = 242 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Positive\rText: Fast answer and a perfect answer because the function I was looking for was in Pleo :D\rCode\ridx = 826 print(\u0026#39;Positive\u0026#39; if y_train.values[idx] \u0026gt; 0 else \u0026#39;Negative\u0026#39;) print(\u0026#39;Text:\u0026#39;, df.iloc[idx][\u0026#34;comments\u0026#34;]) shap.plots.force( explainer.expected_value, shap_values.values[idx], feature_names=vectorizer.get_feature_names(), matplotlib=True, ) Positive\rText: Quick and thorough answer by support agent Sonja!\rFrom several examples it comes that:\naccount word usually means a general problem with account pleo appears in formal reviews, mostly with some claim sonja seems to be a chat bot agent and it gets positive reviews Pretrained neural network Well, we got some new insights using custom model approach, let\u0026rsquo;s see whether the modern NN architecture will be able to unlock even more meaningful take-away\u0026rsquo;s without additional training\nCode\rfrom transformers import pipeline sentiment_transformer_model = pipeline( task=\u0026#34;sentiment-analysis\u0026#34;, model=\u0026#34;cardiffnlp/twitter-roberta-base-sentiment-latest\u0026#34;, return_all_scores=True ) scoring_results = sentiment_transformer_model(X_test[\u0026#34;lemma_text\u0026#34;].to_list()) The drawback of this approach becomes obvious as soon as you start applying it, even inference is taking a lot of time, fingers crossed it\u0026rsquo;s worths it, let\u0026rsquo;s look at the classification quality\nCode\rdef twitter_roberta_predict(scoring_output): scoring_output.sort(key=lambda x: x[\u0026#39;score\u0026#39;], reverse=True) prediction = scoring_output[scoring_output[0][\u0026#39;label\u0026#39;] == \u0026#39;neutral\u0026#39;][\u0026#39;label\u0026#39;] if prediction == \u0026#34;positive\u0026#34;: return 1 elif prediction == \u0026#34;negative\u0026#34;: return -1 else: raise ValueError(\u0026#34;unexpected scoring results\u0026#34;) sentiment_transformer_predictions = [twitter_roberta_predict(scoring) for scoring in scoring_results] print( classification_report(y_test, sentiment_transformer_predictions) ) precision recall f1-score support\r-1 0.19 0.23 0.21 111\r1 0.38 0.32 0.35 165\raccuracy 0.29 276\rmacro avg 0.29 0.28 0.28 276\rweighted avg 0.31 0.29 0.29 276\rIt\u0026rsquo;s kind of expected, given that we use the model which was trained and fine tuned on the texts which have a bit different nature (tweets). To unlock the true power of neural network approach it should be fine tuned to reflect the particular data specific and if you don\u0026rsquo;t have the sufficient amount of data - it should be a red flag not to wasting time with too comprehensive models\nAnyway, let\u0026rsquo;s take a look on how the language model works before pigeonholing it, here we are just some examples, to get a better summary a larger portion of the dataset is needed\nCode\rexplainer = shap.Explainer(sentiment_transformer_model) shap_values = explainer( X_train.loc[X_train[\u0026#34;lemma_text\u0026#34;].apply(lambda x: len(x.split()) \u0026gt; 5), \u0026#34;lemma_text\u0026#34;].sample(3), silent=True ) shap.plots.text(shap_values) ","permalink":"https://npodlozhniy.github.io/posts/feedback-analysis/","summary":"Background Every company has always taken pride in providing excellent customer service, therefore it\u0026rsquo;s crucial as part of ongoing improvements for product to collect and analyse feedback after each customer interaction on support channels\nA top-notch analyst in Hi-Tech industry should be in capable of handle at least base feedback analysis quickly and efficiently and going through this article you will be introduced with the necessary concepts and eventually given the base guide on how to unlock the true power of text data","title":"Customer Feedback Analysis in Python"},{"content":"Introduction This is bold text, and this is emphasized text.\nVisit the Hugo website!\n","permalink":"https://npodlozhniy.github.io/posts/my-first-post/","summary":"Introduction This is bold text, and this is emphasized text.\nVisit the Hugo website!","title":"My First Post"}]